{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b7bc8-fcfc-4a7a-8c68-53c75cbe6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import INV\n",
    "import csv\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1525d3-8271-4393-ad10-3909482a4ca3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebc8cb-5384-437b-b0e2-247f580b057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set a seed for reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(name=\"qwen\"):\n",
    "\n",
    "    path_dict = {\n",
    "        \"qwen\" : \"Qwen/Qwen1.5-7B-Chat\",\n",
    "        \"aya\"  : \"CohereForAI/aya-101\",\n",
    "        \"yi\"   : \"01-ai/Yi-6B-Chat\",\n",
    "    }\n",
    "    \n",
    "    assert name in path_dict, \"unknown model\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\", torch_dtype=\"auto\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "set_seed(42)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(name=\"qwen\")\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9188a78-d209-4444-8a8d-a459e6d62e1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2bd77-1675-4fba-aa36-1004dac031d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9c6ec-817b-43d2-a47e-66366e7378d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse airline tweets\n",
    "\n",
    "def read_tweets(path):\n",
    "    r = csv.DictReader(open(path))\n",
    "    labels = []\n",
    "    confs = []\n",
    "    airlines = []\n",
    "    tdata = []\n",
    "    reasons = []\n",
    "    for row in r:\n",
    "        sentiment, conf, airline, text = row['airline_sentiment'], row['airline_sentiment_confidence'], row['airline'], row['text']\n",
    "        labels.append(sentiment)\n",
    "        confs.append(conf)\n",
    "        airlines.append(airline)\n",
    "        tdata.append(text)\n",
    "        reasons.append(row['negativereason'])\n",
    "\n",
    "    mapping = {'negative': 0, 'positive': 2, 'neutral': 1}\n",
    "    labels = np.array([mapping[x] for x in labels]).astype(int)\n",
    "    \n",
    "    return tdata, labels # labels, confs, airlines, tdata, reasons\n",
    "\n",
    "data, labels = read_tweets('./Tweets.csv')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentences = data\n",
    "parsed_data = list(nlp.pipe(sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf1591-be52-4d98-9ed6-b5c86e62a1b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Named Entity Recognition (NER) test using INVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc43d36-36df-4e43-b6bd-df248f5cb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change location\n",
    "perturb_location_data = Perturb.perturb(parsed_data, Perturb.change_location, nsamples=1000, n=5).data\n",
    "\n",
    "# Change names\n",
    "perturb_names_data = Perturb.perturb(parsed_data, Perturb.change_names, nsamples=1000, n=5).data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cbf06b-1a16-4dc8-938e-0bd5e82a7c26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Robustness using INVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46a744-7826-4a61-929a-0e756f4fdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "def random_string(n):\n",
    "    return ''.join(np.random.choice([x for x in string.ascii_letters + string.digits], n))\n",
    "\n",
    "def random_url(n=6):\n",
    "    return 'https://t.co/%s' % random_string(n)\n",
    "\n",
    "def random_handle(n=6):\n",
    "    return '@%s' % random_string(n)\n",
    "\n",
    "def add_irrelevant(sentence):\n",
    "    urls_and_handles = [random_url(n=6) for _ in range(5)] + [random_handle() for _ in range(5)]\n",
    "    irrelevant_before = ['@airline '] + urls_and_handles\n",
    "    irrelevant_after = urls_and_handles \n",
    "    rets = ['%s %s' % (x, sentence) for x in irrelevant_before ]\n",
    "    rets += ['%s %s' % (sentence, x) for x in irrelevant_after]\n",
    "    return rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bc126-0749-4a0c-a433-d7dd0392df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 1000\n",
    "\n",
    "# Add randomly generated URLs and handles\n",
    "perturb_irrelevant_data = Perturb.perturb(sentences, add_irrelevant, nsamples=N).data\n",
    "\n",
    "# Add typos\n",
    "perturb_punc_data = Perturb.perturb(parsed_data, Perturb.punctuation, nsamples=N).data\n",
    "perturb_typo1_data = Perturb.perturb(sentences, Perturb.add_typos, nsamples=N, typos=1).data\n",
    "perturb_typo2_data = Perturb.perturb(sentences, Perturb.add_typos, nsamples=N, typos=2).data\n",
    "perturb_typo5_data = Perturb.perturb(sentences, Perturb.add_typos, nsamples=N, typos=5).data\n",
    "\n",
    "# Contract or expand contractions\n",
    "perturb_contract_data = Perturb.perturb(sentences, Perturb.contractions, nsamples=N).data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf38bd-f210-412e-b994-6e87534e20bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Negation using Min Func Test (MFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "editor = Editor()\n",
    "editor.tg\n",
    "\n",
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun)\n",
    "\n",
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'weird', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)\n",
    "\n",
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c3bef-eb96-4869-9af0-feb69dce3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple negations\n",
    "N = 1000\n",
    "neg = ['I can\\'t say I', 'I don\\'t', 'I would never say I', 'I don\\'t think I', 'I didn\\'t' ]\n",
    "it1 = ['This', 'That', 'The']\n",
    "nt1 = ['is not', 'isn\\'t']\n",
    "benot1 = ['is not',  'isn\\'t', 'was not', 'wasn\\'t']\n",
    "the1 = ['this', 'that', 'the']\n",
    "\n",
    "# Simple negations of positive statements (should be negative)\n",
    "neg1_negative_data = editor.template('{it} {air_noun} {nt} {pos_adj}.', it=it1, nt=nt1)\n",
    "neg1_negative_data += editor.template('{it} {benot} {a:pos_adj} {air_noun}.', it=it1, benot=benot1)\n",
    "neg1_negative_data += editor.template('{neg} {pos_verb_present} {the} {air_noun}.', neg=neg, the=the1)\n",
    "neg1_negative_data += editor.template('No one {pos_verb_present}s {the} {air_noun}.', neg=neg, the=the1)\n",
    "neg1_negative_data.data = list(np.random.choice(neg1_negative_data.data, N, replace=False))\n",
    "\n",
    "print(len(neg1_negative_data.data))\n",
    "\n",
    "# Simple negations of negative statements (should be positive/neutral)\n",
    "neg1_positive_data = editor.template('{it} {air_noun} {nt} {neg_adj}.', it=it1, nt=nt1)\n",
    "neg1_positive_data += editor.template('{it} {benot} {a:neg_adj} {air_noun}.', it=it1, benot=benot1)\n",
    "neg1_positive_data += editor.template('{neg} {neg_verb_present} {the} {air_noun}.', neg=neg, the=the1)\n",
    "neg1_positive_data += editor.template('No one {neg_verb_present}s {the} {air_noun}.', neg=neg, the=the1)\n",
    "neg1_positive_data.data = list(np.random.choice(neg1_positive_data.data, N, replace=False))\n",
    "\n",
    "print(len(neg1_positive_data.data))\n",
    "\n",
    "# Simple negations of neutral statement (should be neutral)\n",
    "neg1_neutral_data = editor.template('{it} {air_noun} {nt} {neutral_adj}.', it=it1, nt=nt1)\n",
    "neg1_neutral_data += editor.template('{it} {benot} {a:neutral_adj} {air_noun}.', it=it1, benot=benot1)\n",
    "neg1_neutral_data += editor.template('{neg} {neutral_verb_present} {the} {air_noun}.', neg=neg, the=the1)\n",
    "neg1_neutral_data.data = list(np.random.choice(neg1_neutral_data.data, N, replace=False))\n",
    "\n",
    "print(len(neg1_neutral_data.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Negation at the end\n",
    "N = 1000\n",
    "it2 = ['this', 'that', 'the']\n",
    "nt2 = ['is not', 'isn\\'t']\n",
    "the2 = ['this', 'that', 'the']\n",
    "\n",
    "air_noun_it = [x for x in editor.lexicons['air_noun'] if x != 'pilot']\n",
    "\n",
    "# I thought x was positive, but it was not (should be negative)\n",
    "neg2_negative_data = editor.template('I thought {it} {air_noun} would be {pos_adj}, but it {neg}.', air_noun=air_noun_it, neg=['was not', 'wasn\\'t'], it=it2, nt=nt2)\n",
    "neg2_negative_data += editor.template('I thought I would {pos_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=the2)\n",
    "neg2_negative_data.data = list(np.random.choice(neg2_negative_data.data, N, replace=False))\n",
    "\n",
    "print(len(neg2_negative_data.data))\n",
    "\n",
    "# I thought x was negative, but it was not (should be neutral or positive)\n",
    "neg2_positive_data = editor.template('I thought {it} {air_noun} would be {neg_adj}, but it {neg}.', air_noun=air_noun_it, neg=['was not', 'wasn\\'t'], it=it2, nt=nt2)\n",
    "neg2_positive_data += editor.template('I thought I would {neg_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=the2)\n",
    "neg2_positive_data.data = list(np.random.choice(neg2_positive_data.data, N, replace=False))\n",
    "\n",
    "print(len(neg2_positive_data.data))\n",
    "\n",
    "# But it was not (neutral) should still be neutral\n",
    "neg2_neutral_data = editor.template('I thought {it} {air_noun} would be {neutral_adj}, but it {neg}.', air_noun=air_noun_it, neg=['was not', 'wasn\\'t'], it=it2, nt=nt2)\n",
    "neg2_neutral_data += editor.template('I thought I would {neutral_verb_present} {the} {air_noun}, but I {neg}.', neg=['did not', 'didn\\'t'], the=the2)\n",
    "if len(neg2_neutral_data.data) >= N: neg2_neutral_data.data = list(np.random.choice(neg2_neutral_data.data, N, replace=False))\n",
    "\n",
    "print(len(neg2_neutral_data.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242deded",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Negation with neutral in the middle\n",
    "N = 1000\n",
    "neutral =['that I am from Brazil', 'my history with airplanes', 'all that I\\'ve seen over the years', 'the time that I\\'ve been flying', 'it\\'s a Tuesday']\n",
    "neg3 = ['I don\\'t think', 'I can\\'t say', 'I wouldn\\'t say']\n",
    "it3 = ['this', 'that', 'the']\n",
    "be3 = ['is', 'was']\n",
    "i3 = ['I', 'we']\n",
    "the3 = ['this', 'that', 'the']\n",
    "\n",
    "# Negation of positive with neutral stuff in the middle (should be negative)\n",
    "new_neg = neg[:-1]\n",
    "neg3_negative_data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {pos_adj}.', neutral=neutral, neg=neg3, it=it3, be=be3)\n",
    "neg3_negative_data += editor.template('{neg}, given {neutral}, that {it} {be} {a:pos_adj} {air_noun}.', neutral=neutral,  neg=neg3, it=it3, be=be3)\n",
    "neg3_negative_data += editor.template('{neg}, given {neutral}, that {i} {pos_verb_present} {the} {air_noun}.', neutral=neutral,  neg=neg3, i=i3, the=the3)\n",
    "neg3_negative_data.data = list(np.random.choice(neg3_negative_data.data, N, replace=False))\n",
    "\n",
    "print(len(neg3_negative_data.data))\n",
    "\n",
    "# Negation of negative with neutral stuff in the middle (should be positive or neutral)\n",
    "neg3_positive_data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neg_adj}.', neutral=neutral, neg=neg3, it=it3, be=be3)\n",
    "neg3_positive_data += editor.template('{neg}, given {neutral}, that {it} {be} {a:neg_adj} {air_noun}.', neutral=neutral,  neg=neg3, it=it3, be=be3)\n",
    "neg3_positive_data += editor.template('{neg}, given {neutral}, that {i} {neg_verb_present} {the} {air_noun}.', neutral=neutral,  neg=neg3, i=i3, the=the3)\n",
    "neg3_positive_data.data = list(np.random.choice(neg3_positive_data.data, N, replace=False))\n",
    "\n",
    "print(len(neg3_positive_data.data))\n",
    "\n",
    "# Negation of neutral with neutral in the middle, should still neutral\n",
    "neg3_neutral_data = editor.template('{neg}, given {neutral}, that {it} {air_noun} {be} {neutral_adj}.', neutral=neutral, neg=neg3, it=it3, be=be3)\n",
    "neg3_neutral_data += editor.template('{neg}, given {neutral}, that {it} {be} {a:neutral_adj} {air_noun}.', neutral=neutral,  neg=neg3, it=it3, be=be3)\n",
    "neg3_neutral_data += editor.template('{neg}, given {neutral}, that {i} {neutral_verb_present} {the} {air_noun}.', neutral=neutral,  neg=neg3, i=i3, the=the3)\n",
    "neg3_neutral_data.data = list(np.random.choice(neg3_neutral_data.data, N, replace=False))\n",
    "\n",
    "print(len(neg3_neutral_data.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3a269",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SRL using Min Funct Test (MFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Author sentiment is more important\n",
    "N = 1000\n",
    "\n",
    "# Positive sentiment\n",
    "change = [' but', '']\n",
    "templates = ['Some people think you are {neg_adj},{change} I think you are {pos_adj}.', 'I think you are {pos_adj},{change} some people think you are {neg_adj}.',\n",
    "             'I had heard you were {neg_adj},{change} I think you are {pos_adj}.', 'I think you are {pos_adj},{change} I had heard you were {neg_adj}.',]\n",
    "author_positive_data = editor.template(templates, change=change, unroll=True)\n",
    "\n",
    "templates = ['{others} {neg_verb_present} you,{change} I {pos_verb_present} you.', 'I {pos_verb_present} you,{change} {others} {neg_verb_present} you.',]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "author_positive_data += editor.template(templates, others=others, change=change, unroll=True)\n",
    "author_positive_data.data = list(np.random.choice(author_positive_data.data, N, replace=False))\n",
    "\n",
    "print(len(author_positive_data.data))\n",
    "\n",
    "# Negative sentiment\n",
    "change = [' but', '']\n",
    "templates = ['Some people think you are {pos_adj},{change} I think you are {neg_adj}.', 'I think you are {neg_adj},{change} some people think you are {pos_adj}.',\n",
    "             'I had heard you were {pos_adj},{change} I think you are {neg_adj}.', 'I think you are {neg_adj},{change} I had heard you were {pos_adj}.', ]\n",
    "author_negative_data = editor.template(templates, change=change, unroll=True)\n",
    "\n",
    "templates = ['{others} {pos_verb_present} you,{change} I {neg_verb_present} you.', 'I {neg_verb_present} you,{change} {others} {pos_verb_present} you.',]\n",
    "others = ['some people', 'my parents', 'my friends', 'people']\n",
    "author_negative_data += editor.template(templates, others=others, change=change, unroll=True)\n",
    "author_negative_data.data = list(np.random.choice(author_negative_data.data, N, replace=False))\n",
    "\n",
    "print(len(author_negative_data.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a040b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parsing sentiment in question \"yes\" form (0: negative, 1: neutral, 2: positive)\n",
    "N = 1000\n",
    "\n",
    "# Negative question\n",
    "q_yes_positive_data = editor.template('Do I think {it} {air_noun} {be} {pos_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "q_yes_positive_data += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "q_yes_positive_data += editor.template('Did {i} {pos_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "q_yes_positive_data.data = list(np.random.choice(q_yes_positive_data.data, N, replace=False))\n",
    "\n",
    "print(len(q_yes_positive_data.data))\n",
    "\n",
    "# Positive or neutral question\n",
    "q_yes_negative_data = editor.template('Do I think {it} {air_noun} {be} {neg_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "q_yes_negative_data += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "q_yes_negative_data += editor.template('Did {i} {neg_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "q_yes_negative_data.data = list(np.random.choice(q_yes_negative_data.data, N, replace=False))\n",
    "\n",
    "print(len(q_yes_positive_data.data))\n",
    "\n",
    "# Neutral question\n",
    "q_yes_neutral_data = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}? Yes', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "q_yes_neutral_data += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}? Yes', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "q_yes_neutral_data += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}? Yes', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "q_yes_neutral_data.data = list(np.random.choice(q_yes_neutral_data.data, N, replace=False))\n",
    "\n",
    "print(len(q_yes_neutral_data.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa37924",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parsing sentiment in question \"no\" form (0: negative, 1: neutral, 2: positive)\n",
    "N = 1000\n",
    "\n",
    "# Negative question\n",
    "q_no_negative_data = editor.template('Do I think {it} {air_noun} {be} {pos_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "q_no_negative_data += editor.template('Do I think {it} {be} {a:pos_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "q_no_negative_data += editor.template('Did {i} {pos_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "q_no_negative_data.data = list(np.random.choice(q_no_negative_data.data, N, replace=False))\n",
    "\n",
    "print(len(q_no_negative_data.data))\n",
    "\n",
    "# Positive or neutral question\n",
    "q_no_positive_data = editor.template('Do I think {it} {air_noun} {be} {neg_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "q_no_positive_data += editor.template('Do I think {it} {be} {a:neg_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "q_no_positive_data += editor.template('Did {i} {neg_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "q_no_positive_data.data = list(np.random.choice(q_no_positive_data.data, N, replace=False))\n",
    "\n",
    "print(len(q_no_positive_data.data))\n",
    "\n",
    "# Neutral question\n",
    "q_no_neutral_data = editor.template('Do I think {it} {air_noun} {be} {neutral_adj}? No', it=['that', 'this', 'the'], be=['is', 'was'])\n",
    "q_no_neutral_data += editor.template('Do I think {it} {be} {a:neutral_adj} {air_noun}? No', it=['it', 'this', 'that'], be=['is', 'was'])\n",
    "q_no_neutral_data += editor.template('Did {i} {neutral_verb_present} {the} {air_noun}? No', i=['I', 'we'], the=['this', 'that', 'the'])\n",
    "q_no_neutral_data.data = list(np.random.choice(q_no_neutral_data.data, N, replace=False))\n",
    "\n",
    "print(len(q_no_neutral_data.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba52ba7-cf99-42e9-a00b-1480b19825c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ad683-89fb-4764-b40d-48840cf040d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8103d5-b73a-4d7f-b335-eebeb34a3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def response_from_generate(model, messages):\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    output_mapping = {'A' : 0, 'B' : 1, 'C' : 2}\n",
    "    \n",
    "    return output_mapping[response]\n",
    "\n",
    "\n",
    "def response_from_forward(model, messages):\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    output = model.forward(model_inputs.input_ids)\n",
    "    \n",
    "    # idx 32 = A (positive), idx 33 = B (negative), idx 34 = C (neutral)\n",
    "    response = torch.argmax(output.logits[0, -1, 32:35]).item()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def inference_inv(model, data, inference_mode='forward'):\n",
    "    \"\"\"\n",
    "    Perform inference on model using created data samples. The first sentence\n",
    "    in each list of strings is the gold label. inference_mode='generate' means\n",
    "    .generate() is used to create a written response; inference_mode='forward'\n",
    "    means .forward() uses the output logits to determine the response.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_message = \"Give the sentiment of the user's prompt. Please only respond with A (positive), B (negative) or C (neutral).\"\n",
    "    \n",
    "    gold_labels, pred_labels = [], []\n",
    "    \n",
    "    for sentences in tqdm(data):\n",
    "        sentence_labels = []\n",
    "        for i, user_prompt in enumerate(sentences):\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            if inference_mode == 'generate':\n",
    "                response = response_from_generate(model, messages)\n",
    "            elif inference_mode == 'forward':\n",
    "                response = response_from_forward(model, messages)\n",
    "            else:\n",
    "                assert False, 'unknown inference mode'\n",
    "            \n",
    "            if i == 0:\n",
    "                gold_labels.append(response)\n",
    "            else:\n",
    "                sentence_labels.append(response)\n",
    "        \n",
    "        pred_labels.append(sentence_labels)\n",
    "    \n",
    "    return gold_labels, pred_labels\n",
    "\n",
    "\n",
    "def inference_mft(model, data):\n",
    "    \"\"\"\n",
    "    Perform inference on model using created data samples. The first sentence\n",
    "    in each list of strings is the gold label. inference_mode='generate' means\n",
    "    .generate() is used to create a written response; inference_mode='forward'\n",
    "    means .forward() uses the output logits to determine the response.\n",
    "    \"\"\"\n",
    "\n",
    "    system_message = \"Give the sentiment of the user's prompt. Please only respond with A (positive), B (negative) or C (neutral).\"\n",
    "    pred_labels = []\n",
    "    \n",
    "    print(data[0])\n",
    "    \n",
    "    for user_prompt in data:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        response = response_from_forward(model, messages)\n",
    "        pred_labels.append(response)\n",
    "    \n",
    "    return pred_labels\n",
    "\n",
    "\n",
    "# gold_labels, pred_labels = inference_inv(model, perturb_location_data, inference_mode='forward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521bafe3-461d-448c-bf65-c694829dcc07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a43059-eb74-413a-9a5d-5e2462ab2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_inv(gold_labels, pred_labels):\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    for i, sentence_labels in enumerate(pred_labels):\n",
    "        for prompt_label in sentence_labels:\n",
    "            y_pred.append(prompt_label)\n",
    "            y_true.append(gold_labels[i])\n",
    "    \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def evaluate_mft(pred_labels, expected_output=None):\n",
    "    \"\"\"\n",
    "    Give expected output to model (0 = positive, 1 = negative, 2 = neutral).\n",
    "    Can be list if multiple outputs are correct.\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(expected_output) != list:\n",
    "        expected_output = [expected_output]\n",
    "    \n",
    "    y_pred = [1 if label in expected_output else 0 for label in pred_labels]\n",
    "\n",
    "    return sum(y_pred) / len(y_pred)\n",
    "\n",
    "# evaluate_inv(gold_labels, pred_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836d30e-3350-4d6c-81e3-e0edc27d91a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637ad1f-b9b4-427d-acb0-04b837c7d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment NER INV\n",
    "gold_labels, pred_labels = inference_inv(model, perturb_location_data, inference_mode='forward')\n",
    "print(f'Accuracy: {evaluate_inv(gold_labels, pred_labels):.2f}')\n",
    "\n",
    "gold_labels, pred_labels = inference_inv(model, perturb_names_data, inference_mode='forward')\n",
    "print(f'Accuracy: {evaluate_inv(gold_labels, pred_labels):.2f}')\n",
    "\n",
    "# Sentiment Robustness INV\n",
    "gold_labels, pred_labels = inference_inv(model, perturb_irrelevant_data, inference_mode='forward')\n",
    "print(f'Accuracy: {evaluate_inv(gold_labels, pred_labels):.2f}')\n",
    "\n",
    "gold_labels, pred_labels = inference_inv(model, perturb_punc_data, inference_mode='forward')\n",
    "print(f'Accuracy: {evaluate_inv(gold_labels, pred_labels):.2f}')\n",
    "\n",
    "gold_labels, pred_labels = inference_inv(model, perturb_typo1_data, inference_mode='forward')\n",
    "print(f'Accuracy: {evaluate_inv(gold_labels, pred_labels):.2f}')\n",
    "\n",
    "gold_labels, pred_labels = inference_inv(model, perturb_typo2_data, inference_mode='forward')\n",
    "print(f'Accuracy: {evaluate_inv(gold_labels, pred_labels):.2f}')\n",
    "\n",
    "gold_labels, pred_labels = inference_inv(model, perturb_typo5_data, inference_mode='forward')\n",
    "print(f'Accuracy: {evaluate_inv(gold_labels, pred_labels):.2f}')\n",
    "\n",
    "gold_labels, pred_labels = inference_inv(model, perturb_contract_data, inference_mode='forward')\n",
    "print(f'Accuracy: {evaluate_inv(gold_labels, pred_labels):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715650e3-10a3-42c9-bd38-8a702d5e0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Negation MFT\n",
    "# Level 1\n",
    "pred_labels = inference_mft(model, neg1_negative_data.data)\n",
    "accuracy = evaluate_mft(pred_labels, expected_output=1)\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "\n",
    "pred_labels = inference_mft(model, neg1_positive_data.data)\n",
    "accuracy = evaluate_mft(pred_labels, expected_output=[0, 2])\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "\n",
    "pred_labels = inference_mft(model, neg1_neutral_data.data)\n",
    "accuracy = evaluate_mft(pred_labels, expected_output=2)\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "\n",
    "# Level 2\n",
    "pred_labels = inference_mft(model, neg2_negative_data.data)\n",
    "accuracy = evaluate_mft(pred_labels, expected_output=1)\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "\n",
    "pred_labels = inference_mft(model, neg2_positive_data.data)\n",
    "accuracy = evaluate_mft(pred_labels, expected_output=[0, 2])\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "\n",
    "pred_labels = inference_mft(model, neg2_neutral_data.data)\n",
    "accuracy = evaluate_mft(pred_labels, expected_output=2)\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "\n",
    "# Level 3\n",
    "pred_labels = inference_mft(model, neg3_negative_data.data)\n",
    "accuracy = evaluate_mft(pred_labels, expected_output=1)\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "\n",
    "pred_labels = inference_mft(model, neg3_positive_data.data)\n",
    "accuracy = evaluate_mft(pred_labels, expected_output=[0, 2])\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "\n",
    "pred_labels = inference_mft(model, neg3_neutral_data.data)\n",
    "accuracy = evaluate_mft(pred_labels, expected_output=2)\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ca53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment SRL MFT\n",
    "\n",
    "# TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atcs",
   "language": "python",
   "name": "atcs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
